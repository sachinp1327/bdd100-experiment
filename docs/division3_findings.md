# Division 3 Findings: Post‑Training Evaluation Diagnostics

This document summarizes the **post‑training evaluation diagnostics** for the trained Faster R‑CNN model and connects results to the dataset analysis (Division 1). The diagnostics are generated by the Division 3 module using COCO‑style predictions and validation ground truth.

Primary artifacts used:
- `outputs/eval_diagnostics/overall_metrics.json`
- `outputs/eval_diagnostics/per_class_metrics.json`
- `outputs/eval_diagnostics/attribute_metrics.json`
- `outputs/eval_diagnostics/samples.json`
- `outputs/eval_diagnostics/renders/` (visual overlays)
- `outputs/val/summary.json` (Division 1 analysis for context)

---

**Overall Quantitative Performance (Validation)**

From `overall_metrics.json`:
- **AP**: **0.2649**
- **AP50**: **0.5154**
- **AP75**: **0.2347**
- **AP_small**: **0.1163**
- **AP_medium**: **0.3141**
- **AP_large**: **0.4502**
- **AR_1 / AR_10 / AR_100**: **0.1917 / 0.3604 / 0.3807**

Key takeaway:
- Performance is **strongest on large objects**, significantly weaker on small objects.

---

**Class‑Level Strengths and Weaknesses**

Top classes by AP (from `per_class_metrics.json`):
- **car**: AP **0.460**
- **bus**: AP **0.393**
- **truck**: AP **0.372**
- **traffic sign**: AP **0.349**
- **person**: AP **0.292**

Bottom classes by AP:
- **train**: AP **0.000** (very rare class)
- **motor**: AP **0.174**
- **bike**: AP **0.187**
- **rider**: AP **0.188**
- **traffic light**: AP **0.234**

Interpretation:
- Classes with **higher frequency and larger box sizes** perform best.
- **Rare and small classes** (train, motor, bike, rider) are the weakest.
- `traffic light` has lower AP75, indicating **localization difficulty** even when AP50 is moderate.

---

**Where the Model Works Well**

Quantitative indicators:
- High AP on **car**, **bus**, **truck**.
- **AP_large** (0.450) is substantially higher than AP_small (0.116).

Qualitative patterns:
- Clear, daylight images with moderate clutter show strong detections.
- Large objects in the foreground are consistently localized and classified.

Attribute analysis example (car, from `attribute_metrics.json`):
- **Daytime** and **overcast** conditions yield higher F1 than night.
- **Residential / city street** scenes perform better than highway.

---

**Where the Model Struggles (and Why)**

1. **Small objects and far‑distance targets**
   - Division 1 shows **~55%** of boxes are small and **~82%** are far.
   - This aligns with low **AP_small (0.116)** and lower performance on tiny classes.

2. **Rare classes**
   - `train` has **15 boxes in val** (and 136 in train).
   - Extremely low support leads to unstable AP (often 0).

3. **High clutter and occlusion**
   - Division 1: **~88%** of images are high‑clutter.
   - Occlusion affects **~47%** of boxes.
   - Crowded scenes increase confusion and reduce precise localization.

4. **Night / adverse conditions**
   - Attribute metrics show lower F1 in **night** and **rainy/foggy** cases.
   - Lower contrast and noise reduce detection precision.

---

**Qualitative Evidence (Visualization)**

The diagnostics produce visual overlays with:
- **GT boxes** (green) and **Pred boxes** (red).

Use the following for evidence in the report:
- Best/worst examples by class:
  - `outputs/eval_diagnostics/renders/<class>/best/*.jpg`
  - `outputs/eval_diagnostics/renders/<class>/worst/*.jpg`
- The paired metadata is in:
  - `outputs/eval_diagnostics/samples.json`

These examples show:
- **True positives** in clean scenes.
- **False positives / missed detections** in clutter, heavy occlusion, or tiny objects.

---

**Why These Evaluation Metrics Were Chosen**

We use the COCO evaluation protocol because it provides:
- **AP (IoU 0.50:0.95)**: combines detection quality + localization accuracy.
- **AP50 vs AP75**: separates coarse vs precise localization quality.
- **AP_small / AP_medium / AP_large**: reveals scale sensitivity, crucial for BDD100K.
- **AR (Average Recall)**: measures coverage of ground‑truth objects independent of precision.

These metrics are standard and allow comparison with prior work and benchmarks.

---

**Connecting Division 1 Analysis to Model Performance**

Findings that directly explain evaluation outcomes:
- **Small objects dominate** (55% of boxes) → low AP_small.
- **Far‑distance objects dominate** (~82%) → poor detection of tiny targets.
- **High clutter (88%)** → more confusion and overlap errors.
- **High occlusion (47%)** → missed detections and localization errors.
- **Class imbalance** (train/motor/bike/rider rare) → unstable AP.

This confirms that the model struggles most where the dataset is hardest.

---

**Suggested Improvements**

Model‑level improvements:
- Train longer (24+ epochs) or use a stronger backbone (ResNet‑101).
- Increase input resolution to better resolve small objects.
- Use multi‑scale training or more aggressive scale jittering.
- Adjust RPN anchor sizes/aspect ratios to better cover small objects.

Data‑level improvements:
- Collect or oversample rare classes (train, motor, rider, bike).
- Add targeted augmentation for occlusion and night/rain conditions.
- Rebalance training batches to include more small‑object examples.

Evaluation‑level improvements:
- Track per‑attribute performance in time (night vs day, weather, scene).
- Use difficulty‑aware splits (occluded vs non‑occluded).

---

**Summary**

The model performs well on large, frequent classes and struggles on rare and small classes. This is consistent with dataset statistics: a large fraction of objects are small, far, and occluded in high‑clutter scenes. The diagnostics pipeline provides both quantitative and qualitative evidence and highlights clear paths for improving both model design and data coverage.
